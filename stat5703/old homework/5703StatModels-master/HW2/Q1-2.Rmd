---
title: "STAT5703 HW2 Ex1"
author: "Chao Huang (ch3474), Wancheng Chen (wc2687), Chengchao Jin (cj2628)"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(Amelia)
library(SMPracticals)
```

##Exercise 1.
```{r}
library(readr)
cancer <- read_table2("cancer.txt")[,2:7]
```

####1.
$Answer:$ It is reasonable to say the right censoring in this data set is random because the patients who are right censored(with d=0) have various censoring time. So censoring time is not fixed.

####2.
```{r}
library(ggfortify)
library(survival)
library(dplyr)
library(ggplot2)
autoplot(survfit(Surv(t,d)~arm,data=cancer),conf.int=TRUE,col=c(2,3),title="Kaplan-Meier estimates for cancer data")
```
* The plot seems to indicate a difference between two groups. Treatment B seems to be more efficient. 

####3.
```{r}
cancer$arm<-factor(cancer$arm)
cancer0<-cancer%>%filter(cancer$arm=="A")
cancer1<-cancer%>%filter(cancer$arm=="B")
fit_exp<-survreg(Surv(t,d)~arm,data = cancer,dist = "exponential")
summary(fit_exp)
```
* From the summary table, we can see that the estimated value of armB is 0.759>0. So we can know that the treatment B is efficient and can increase the survival time. It agrees with my intuition from last point.

####4.
* From the summary table in point 3, we get Chisq=10.41 which is the likelihood ratio test between two groups. Its p value is 0.0013 so there is strong evidence that the model which includes both armA and armB is better than intercept only and thus the effect of treatment B is significant. This conclusion does not depend on parametric model assumptions

####5.
```{r}
plot(survfit(Surv(t,d)~arm,data=cancer),conf.int=TRUE,col=c(2,3),main="Exponential v. K-M fits")
x <- seq(from=0,to=2500,by=1)
lines(x,1-pexp(x,exp(-coef(fit_exp)[1])),col="darkred",lwd=2)
lines(x,1-pexp(x,exp(-sum(coef(fit_exp)))),col="darkgreen",lwd=2)
```
* From the graph, we can see that exponential model does great to fit. The lines lie inside the corresponding group's confidence interval. It does extremely good for fitting group A and it correctly goes to zero after group A is not cencored. But it is not really good when fitting group B as it the fitted line goes outside of confidence interval after around time=1500 and it also goes to zero even though group B is still right censored.

####6.
```{r}
fit_wei12<-survreg(Surv(t,d)~arm,data = cancer0,dist = "weibull")
fit_wei22<-survreg(Surv(t,d)~arm,data = cancer1,dist = "weibull")
gamma12=1/exp(fit_wei12$scale)
gamma22=1/exp(fit_wei22$scale)
plot(survfit(Surv(t,d)~arm,data=cancer),conf.int=TRUE,col=c(2,3),main="Weibull v. K-M fits")
x <- seq(from=0,to=2500,by=1)
lines(x,1-pweibull(x,gamma12,exp(coef(fit_wei12)[1])),col="darkred",lwd=2)
lines(x,1-pweibull(x,gamma22,exp(coef(fit_wei22)[1])),col="darkgreen",lwd=2)
1-pchisq(2*(fit_wei12$loglik[2]+fit_wei22$loglik[2]-fit_exp$loglik[2]),df=1)
```
* This model fits the data better by simply looking at the two fitted lines in each graph. The evidence against the relevence on an exponential model is obtained by calculating likelihood ratio test between Weibull and exponential models. Then we can get a p-value=0.0324<0.05. It means the Weibull model is better than the exponential model.

## Exercise 2

#### Question 1

```{r}
data <- read.table('scores.txt', header = TRUE, sep = "", dec = ".")
colnames(data) <- c('A','B','C','D','E')
```

```{r}
# Complete case analysis.
cov_1 <- cov(data,use="complete.obs")
cov_1
```

```{r}
# Available case analysis.
cov_2 <- cov(data,use="pairwise.complete.obs")
cov_2
```

```{r}
# Mean imputation
data_mean <- sapply(data, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))
cov_3 <- cov(data_mean,use="complete.obs")
cov_3
```
```{r}
# Mean inputation with bootstrap
cov_4<-matrix(rep(0,25),ncol=5)
for(i in 1:200){
  ind<-sample(nrow(data),22,replace=TRUE)
  temp <- sapply(data[ind,], function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))
  cov_4 <- cov_4 + cov(temp,use="complete.obs")
}
cov_4/200
```

```{r}
# The EM-algorithm
a.out <- amelia(data,m=1,boot.type='none')
cov_5 <- cov(a.out$imputations$imp1,use='complete.obs')
cov_5
```

* Using mean imputation or mean imputation with bootstrap to fill the missing data has smaller covariance matrix value comparing to the other three methods.
* Complete case analysis and em-algorithm show the negative correlation between variable A & B while the other methods do not.
* The results of covariance matrix using methods of mean inputation and mean inputation with bootstrap are quite close.

#### Question 2

Because $\sqrt{n}(\hat \lambda_1-\lambda_1)\to N(0,2\lambda^2)$, we can get that $\hat \lambda_1 \sim N(\lambda_1,\frac{2\lambda^2}{n})$.
$$P[-Z_{1-\frac{\alpha}{2}} \leq \frac{\hat \lambda_1-\lambda_1}{\sqrt{\frac{2\lambda_1^2}{n}}} \leq Z_{1-\frac{\alpha}{2}}]=1-\alpha$$
Try to calculate the lower bound for $\lambda_1$
$$\hat \lambda_1-\lambda_1 \leq \frac{Z_{1-\frac{\alpha}{2}}\sqrt{2}\lambda_1}{\sqrt{n}}$$
$$\lambda_1 \geq \frac{\hat\lambda_1}{1+\sqrt{\frac{2}{n}}Z_{1-\frac{\alpha}{2}}}$$
In the same way, we can calculate the upper bound for $\lambda_1$
$$\lambda_1 \leq \frac{\hat\lambda_1}{1-\sqrt{\frac{2}{n}}Z_{1-\frac{\alpha}{2}}}$$
So the confidence interval for $\lambda_1$ is:
$$\begin{aligned}
\lambda_1 &\in [\frac{\hat\lambda_1}{1+\sqrt{\frac{2}{n}}Z_{1-\frac{\alpha}{2}}},\frac{\hat\lambda_1}{1-\sqrt{\frac{2}{n}}Z_{1-\frac{\alpha}{2}}}]
\end{aligned}$$
```{r}
get_interval <- function(lambda) {
  mu = lambda
  sd <- sqrt(2*lambda*lambda)
  print(paste0('Left: ',mu/(1+sqrt(2/nrow(data))*qnorm(0.975)),
               ' right: ',mu/(1-sqrt(2/nrow(data))*qnorm(0.975))))
}
get_interval(max(eigen(cov_1)$value))
get_interval(max(eigen(cov_2)$value))
get_interval(max(eigen(cov_3)$value))
get_interval(max(eigen(cov_4)$value))
get_interval(max(eigen(cov_5)$value))
```

* Using mean imputation with bootstrap to fill the missing data will generate much greater confidence interval range than the other four methods, Meanwhile, the value of lower bound and upper bound becomes much different than the others. Thus, we may not use mean imputation with bootstrape to fill the missing data.
* Using em-algorithm to fill in the missing data, it can generate smaller range of confidence interval than using complete case analysis or available case analysis methods. It seems like a good method to fill in the missing data.

#### Question 3

```{r}
pvar<-cov(mathmarks)
pvar
get_interval(max(eigen(pvar)$value))
```

* Using available case analysis method to fill in the missing data can generate more close sample covariance matrix and confidence interval of $\lambda_1$ to the results of true complete data comparing to other methods.
* Imputation methods for the missing data, especially em-algorithm method, may be affected due to the insufficient data as the input data has only 22 rows.

#### Question 4

For Missing data, we can construct:
$$X_i=\begin{bmatrix}
X_{io} \\
X_{im} \\
\end{bmatrix}
,
X_iX_i'=\begin{bmatrix}
X_{io}X_{io}'&X_{io}X_{im}' \\
X_{im}X_{io}'&X_{im}X_{im}' \\
\end{bmatrix}$$
Let,
$$\mu^{(k)}=\begin{bmatrix}
\mu_{io}^{(k)} \\
\mu_{im}^{(k)} \\
\end{bmatrix}
,
\Sigma^{(k)}=\begin{bmatrix}
\Sigma_{ioo}^{(k)}&\Sigma_{iom}^{(k)} \\
\Sigma_{imo}^{(k)}&\Sigma_{imm}^{(k)} \\
\end{bmatrix}$$
Then, for E-step:
$$E(X_i|X_io)=\begin{bmatrix}
X_{io} \\
E(X_{im}|X_{io}) \\
\end{bmatrix}
,
E(X_iX_i'|X_{io})=\begin{bmatrix}
X_{io}X_{io}'&X_{io}E(X_{im}'|X_{io}) \\
E(X_{im}|X_{io})X_{io}'&E(X_{im}X_{im}'|X_{io}) \\
\end{bmatrix}\\
E(X_{im}|X_{io})=\mu_{im}^{(k)}+\Sigma_{imo}^{(k)}(\Sigma_{ioo}^{(k)})^{-1}(X_{io}-\mu_{io}^{(k)})\\
E(X_{im}X_{im}'|X_{io})=(\Sigma_{imm}^{(k)}-\Sigma_{imo}^{(k)}(\Sigma_{ioo}^{(k)})^{-1}\Sigma_{iom}^{(k)})=C_{imm}^{(k)}$$
Then, for M-step:
$$\mu^{(k+1)}=\frac{1}{n}\sum_{i=1}^nE(X_i|X_{io})=0,
\Sigma^{(k+1)}=\frac{1}{n}\sum_{i=1}^nE(X_iX_i'|X_{io})-\mu^{(k+1)}\mu^{(k+1)'}$$
To simplify using the information above, we can get:
$$\mu^{(k+1)}=\sum_{i=1}^n(\hat{X_i}-\mu)=0,\ 
\Sigma^{(k+1)}=\sum_{i=1}^n(\Sigma-(\hat{X_i}-\mu)(\hat{X_i}-\mu)^T-C_i^{(k)})=0$$
