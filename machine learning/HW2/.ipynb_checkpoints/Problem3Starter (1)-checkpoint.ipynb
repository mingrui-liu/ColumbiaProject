{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import special\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the color map to be used for plotting.\n",
    "cmap = colors.LinearSegmentedColormap(\n",
    "    'red_blue_classes',\n",
    "    {'red': [(0, 1, 1), (1, 0.7, 0.7)],\n",
    "     'green': [(0, 0.7, 0.7), (1, 0.7, 0.7)],\n",
    "     'blue': [(0, 0.7, 0.7), (1, 1, 1)]})\n",
    "plt.cm.register_cmap(cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the problem 3 file.\n",
    "from numpy import genfromtxt\n",
    "my_data = genfromtxt('logistic_regression.csv', delimiter=',', skip_header=1)\n",
    "print(my_data.shape)\n",
    "X = my_data[:, [0, 1]]\n",
    "y = my_data[:, 2]\n",
    "\n",
    "# Divide the data into two classes for plotting.\n",
    "X0 = my_data[numpy.where(my_data[:, 2] == 0)]\n",
    "X1 = my_data[numpy.where(my_data[:, 2] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements the natural log of the logistic function on $w^T x$.\n",
    "def log_logistic_prob(iterate, x):\n",
    "    t = numpy.dot(iterate[1:], x) + iterate[0]\n",
    "    if t < -33:\n",
    "        return t\n",
    "    elif t < -18:\n",
    "        return t - numpy.exp(t)\n",
    "    elif t < 37:\n",
    "        return -numpy.log1p(numpy.exp(- t ))\n",
    "    else:\n",
    "        return -numpy.exp(-t)\n",
    "\n",
    "# Implements the logistic function on $w^T x$.\n",
    "def logistic_prob(iterate, x):\n",
    "    t = numpy.dot(iterate[1:], x) + iterate[0]\n",
    "    if t < -33.3:\n",
    "        return numpy.exp(t)\n",
    "    elif t <= -18:\n",
    "        return numpy.exp(t - numpy.exp(t))\n",
    "    elif t <= 37:\n",
    "        return numpy.exp(-numpy.log1p(numpy.exp(-t)))\n",
    "    else:\n",
    "        return numpy.exp(-numpy.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hUJ_acF1jAb"
   },
   "outputs": [],
   "source": [
    "# Evaluates the logistic function on a set of grid points.\n",
    "def logistic_prob_grid(iterate, grids):\n",
    "    return numpy.array([logistic_prob(iterate, x) for x in grids ])\n",
    "\n",
    "# Takes the logistic probability and thresholds to outputs a classification label.\n",
    "def logistic_pred(iterate, x):\n",
    "    return 1.0 if logistic_prob(iterate, x) > 0.5 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements the negative logistic regression objective. Modify me!\n",
    "def negative_log_likelihood(iterate, X, y, regularization):\n",
    "        \n",
    "    # The accumulated objective value.\n",
    "    obj_val = 0.0\n",
    "        \n",
    "    # Loop over each (x, y) pair.\n",
    "    for i, (x_vec, y) in enumerate(zip(X, y)):\n",
    "            \n",
    "        # Dot product $w^T x$.            \n",
    "        predict = iterate[0] + numpy.dot(iterate[1:], x_vec)\n",
    "            \n",
    "        # Accumulate the objective value contribution from this (x, y) pair.\n",
    "        obj_val += (- (1 - y) * predict + log_logistic_prob(iterate, x_vec) )\n",
    "        \n",
    "    # Subtract the regularization parameter.\n",
    "    return - obj_val + regularization * numpy.dot(iterate[1:], iterate[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements the logistic regression gradient. Modify me!\n",
    "def gradient_negative_log_likelihood(iterate, X, y, regularization):\n",
    "    gradient = numpy.zeros(3)\n",
    "    \n",
    "    # Loop over each (x, y) pair. \n",
    "    for i, (x_vec, y) in enumerate(zip(X, y)):\n",
    "        \n",
    "        # Dot product $w^T x$.            \n",
    "        predict = iterate[0] + numpy.dot(iterate[1:], x_vec)\n",
    "        \n",
    "        if predict > 0.0:\n",
    "            factor = ((y - 1) + y * numpy.exp(-predict)) / ( 1 + numpy.exp(-predict) )\n",
    "        else:\n",
    "            factor = ((y - 1) * numpy.exp(predict) + y ) / ( 1 + numpy.exp(predict) )\n",
    "        gradient[0] -= factor\n",
    "        gradient[1:] -= factor * x_vec\n",
    "        \n",
    "    # Regularize gradient.\n",
    "    gradient[1:] += 2 * regularization * iterate[1:]\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the data with the decision boundary.\n",
    "def plot_data(X, y, iterate, regularization):\n",
    "    y_pred = [logistic_pred(iterate, x) for x in X]\n",
    "    tp = (y == y_pred)  # True Positive\n",
    "    tp0, tp1 = tp[y == 0], tp[y == 1]\n",
    "    X0, X1 = X[y == 0], X[y == 1]\n",
    "    X0_tp, X0_fp = X0[tp0], X0[~tp0]\n",
    "    X1_tp, X1_fp = X1[tp1], X1[~tp1]\n",
    "\n",
    "    # class 0: dots\n",
    "    plt.scatter(X0_tp[:, 0], X0_tp[:, 1], marker='.', color='red')\n",
    "    plt.scatter(X0_fp[:, 0], X0_fp[:, 1], marker='x',\n",
    "                s=20, color='#990000')  # dark red\n",
    "\n",
    "    # class 1: dots\n",
    "    plt.scatter(X1_tp[:, 0], X1_tp[:, 1], marker='.', color='blue')\n",
    "    plt.scatter(X1_fp[:, 0], X1_fp[:, 1], marker='x',\n",
    "                s=20, color='#000099')  # dark blue\n",
    "\n",
    "    # class 0 and 1 : areas\n",
    "    nx, ny = 200, 200\n",
    "    x_min, x_max = (-10, 10)\n",
    "    y_min, y_max = (-10, 10)\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(-10, 10)\n",
    "    xx, yy = numpy.meshgrid(numpy.linspace(x_min, x_max, nx),\n",
    "                         numpy.linspace(y_min, y_max, ny))\n",
    "\n",
    "    Z = logistic_prob_grid(iterate, numpy.c_[xx.ravel(), yy.ravel()])    \n",
    "    Z = Z.reshape(xx.shape)\n",
    "    neg_log_likelihood = negative_log_likelihood(iterate, X, y, regularization)\n",
    "    plt.title(\n",
    "        'Negative log likelihood: ' + str(neg_log_likelihood))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap='red_blue_classes',\n",
    "                   norm=colors.Normalize(0., 1.), zorder=0)\n",
    "    \n",
    "    # Plot my linear decision boundary here!\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for each different regularization value by calling your optimization routine for\n",
    "# different values."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Module 3 Lecture.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
