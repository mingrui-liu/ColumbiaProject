{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello and welcome to decision trees!\n",
    "Decision trees are often pretty effect learning algorithms, and certainly serve as an interesting technical exercise in data preprocessing and recursion. This notebook will walk you through some of the basic notions of how the implementation should be executed, and also give you a chance to write some of your own code.\n",
    "\n",
    "Suggested reading before you start: https://www.cs.princeton.edu/courses/archive/spring07/cos424/papers/mitchell-dectrees.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the notebook you'll see TODO tags in the comments. This is where you should insert your own code to make the functions work! If you get stuck, we encourage you to come to office hours. You can also try to look at APIs and documentation online to try to get a sense how certain methods work. If you take inspiration from any source online other than official documentation, please be sure to cite the resource! Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using decision trees to classify if a banknote is fradulent (class 1) or not fradulent (class 0). Download data from https://archive.ics.uci.edu/ml/datasets/banknote+authentication#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO YOUR CODE HERE\n",
    "def import_data(split = 0.8, shuffle=False):\n",
    "    \"\"\"Read in the data, split it by split percentage into train and test data, \n",
    "    and return X_train, y_train, X_test, y_test as numpy arrays\"\"\"\n",
    "    # TODO\n",
    "    \n",
    "    data = pd.read_csv('data_banknote_authentication.txt', header = None)\n",
    "    data = np.array(data)\n",
    "    if shuffle == True:\n",
    "        data = np.random.shuffle(data)\n",
    "        \n",
    "    X = data[:,:-1]\n",
    "    y = data[:,-1]\n",
    "    y = y.astype(int)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    print(\"data imported\")\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data imported\n"
     ]
    }
   ],
   "source": [
    "a,b,c,d = import_data(split = 0.8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"Each node of our decision tree will hold values such as left and right children, \n",
    "    the data and labels being split on, the threshold value & index in the dataframe for a particular feature,\n",
    "    and the uncertainty measure for this node\"\"\"\n",
    "    def __init__(self, data, labels, depth):\n",
    "        \"\"\"\n",
    "        data: X data\n",
    "        labels: y data\n",
    "        depth: depth of tree\n",
    "        \"\"\"\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.depth = depth\n",
    "\n",
    "        self.threshold = None # threshold value\n",
    "        self.threshold_index = None # threshold index\n",
    "        self.feature = None # feature as a NUMBER (column number)\n",
    "        self.label = None # y label\n",
    "        self.uncertainty = None # uncertainty value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, K=5, verbose=False):\n",
    "        \"\"\"\n",
    "        K: number of features to split on \n",
    "        \"\"\"\n",
    "        self.root = None\n",
    "        self.K = K\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def buildTree(self, data, labels, metric = \"entropy\"):\n",
    "        \"\"\"Builds tree for training on data. Recursively called _buildTree\"\"\"\n",
    "        self.root = Node(data, labels, 0)\n",
    "        if self.verbose:\n",
    "            print(\"Root node shape: \", data.shape, labels.shape)\n",
    "        self._buildTree(self.root, metric = metric)\n",
    "\n",
    "    def _buildTree(self, node, metric = \"entropy\"):\n",
    "             \n",
    "        # get uncertainty measure and feature threshold\n",
    "        node.uncertainty = self.get_uncertainty(node.labels)\n",
    "        self.get_feature_threshold(node, metric = metric)\n",
    "            \n",
    "        index = node.data[:, node.feature].argsort()  # sort feature for return\n",
    "        node.data = node.data[index]\n",
    "        node.labels = node.labels[index]\n",
    "        \n",
    "        # check label distribution.\n",
    "        label_distribution = np.bincount(node.labels)\n",
    "        majority_label = node.labels[0] if len(label_distribution) == 1 else np.argmax(label_distribution)\n",
    "            \n",
    "        if self.verbose:\n",
    "            print(\"Node uncertainty: %f\" % node.uncertainty)\n",
    "    \n",
    "        # Split left and right if threshold is not the min or max of the feature or every point has the\n",
    "        # same label.\n",
    "        if node.threshold_index == 0 or node.threshold_index == node.data.shape[0] or \\\n",
    "            len(label_distribution) == 1:\n",
    "            node.label = majority_label\n",
    "        else:\n",
    "            node.left = Node(node.data[:node.threshold_index], node.labels[:node.threshold_index], node.depth + 1)\n",
    "            node.right = Node(node.data[node.threshold_index:], node.labels[node.threshold_index:], node.depth + 1)            \n",
    "            node.data = None\n",
    "            node.labels = None\n",
    "                        \n",
    "            # If in last layer of tree, assign predictions\n",
    "            if node.depth == self.K:\n",
    "                if len(node.left.labels) == 0:\n",
    "                    node.right.label = np.argmax(np.bincount(node.right.labels))\n",
    "                    node.left.label = 1 - node.right.label\n",
    "                elif len(node.right.labels) == 0:\n",
    "                    node.left.label = np.argmax(np.bincount(node.left.labels))\n",
    "                    node.right.label = 1 - node.left.label\n",
    "                else:\n",
    "                    node.left.label = np.argmax(np.bincount(node.left.labels))\n",
    "                    node.right.label = np.argmax(np.bincount(node.right.labels))\n",
    "                return\n",
    "\n",
    "            else: # Otherwise continue training the tree by calling _buildTree\n",
    "                self._buildTree(node.left)\n",
    "                self._buildTree(node.right)\n",
    "\n",
    "    def predict(self, data_pt):\n",
    "        return self._predict(data_pt, self.root)\n",
    "\n",
    "    def _predict(self, data_pt, node):\n",
    "        feature = node.feature\n",
    "        threshold = node.threshold\n",
    "        if node.label is not None:\n",
    "            return node.label\n",
    "        elif data_pt[node.feature] < node.threshold:\n",
    "            return self._predict(data_pt, node.left)\n",
    "        elif data_pt[node.feature] >= node.threshold:\n",
    "            return self._predict(data_pt, node.right)\n",
    "\n",
    "    def get_feature_threshold(self, node, metric = \"entropy\"):\n",
    "        \"\"\" TODO Find the feature that gives the largest information gain. Update node.threshold, \n",
    "        node.threshold_index, and node.feature (a number representing the feature. e.g. 2nd column feature would be 1)\n",
    "        Make sure to sort the columns of data before you try to find the threshold index (look at numpy argsort) and set the values\n",
    "        for node.threshold, node.threshold_index, and node.feature\n",
    "        return: None\n",
    "        \"\"\"\n",
    "        node.threshold = 0\n",
    "        node.threshold_index = 0\n",
    "        node.feature = 0\n",
    "        # TODO YOUR CODE HERE\n",
    "        n = node.labels.shape[0]\n",
    "        gain = 0\n",
    "        fea = 0\n",
    "        for k in range(node.data.shape[1]):\n",
    "            node.feature = k\n",
    "            for i in range(n):\n",
    "                new_gain = self.getInfoGain(node, i, metric = metric)\n",
    "                if new_gain > gain:\n",
    "                    gain = new_gain\n",
    "                    node.threshold_index = i\n",
    "                    fea = k\n",
    "        node.feature = fea\n",
    "        node.threshold = sorted(node.data[:,node.feature])[node.threshold_index]\n",
    "\n",
    "    def getInfoGain(self, node, split_index, metric = \"entropy\"):\n",
    "        \"\"\"\n",
    "        TODO Get information gain using the variables in the parameters, \\\n",
    "        split_index: index in the feature column that you are splitting the classes on\n",
    "        return: information gain (float)\n",
    "        \"\"\"\n",
    "        # TODO YOUR CODE HERE\n",
    "        n = len(node.labels)\n",
    "        nl = split_index + 1\n",
    "        nr = n - nl\n",
    "        index = node.data[:, 1].argsort() \n",
    "        qm = self.get_uncertainty(node.labels, metric = metric)\n",
    "        ql = self.get_uncertainty(node.labels[index[:split_index]], metric = metric)\n",
    "        qr = self.get_uncertainty(node.labels[index[split_index:]], metric = metric)\n",
    "        return qm - (nl / n * ql) - (nr / n * qr)\n",
    "\n",
    "    def get_uncertainty(self, labels, metric=\"entropy\"):\n",
    "        \"\"\"\n",
    "        TODO Get uncertainty. Implement entropy AND gini index metrics. \n",
    "        np.bincount(labels) and labels.shape might be useful here\n",
    "        return: uncertainty (float)\n",
    "        \"\"\"\n",
    "        \n",
    "        if labels.shape[0] == 0:\n",
    "            return 1\n",
    "        # TODO YOUR CODE HERE\n",
    "        else:\n",
    "            p = np.bincount(labels) / labels.shape[0]\n",
    "            if 0 not in p:\n",
    "                if metric == \"entropy\":\n",
    "                    return - sum(p * np.log(p))\n",
    "                elif metric == \"gini\":\n",
    "                    return sum(p * (1-p))\n",
    "                else:\n",
    "                    return 1\n",
    "            else:\n",
    "                return 1\n",
    "\n",
    "    def printTree(self):\n",
    "        \"\"\"Prints the tree including threshold value and feature name\"\"\"\n",
    "        self._printTree(self.root)\n",
    "\n",
    "    def _printTree(self, node):\n",
    "        if node is not None:\n",
    "            if node.label is None:\n",
    "                print(\"\\t\" * node.depth, \"(%d, %d)\" % (node.threshold, node.feature))\n",
    "            else:\n",
    "                print(\"\\t\" * node.depth, node.label)\n",
    "            self._printTree(node.left)\n",
    "            self._printTree(node.right)\n",
    "\n",
    "    def homework_evaluate(self, X_train, labels, X_test, y_test):\n",
    "        n = X_train.shape[0]\n",
    "\n",
    "        count = 0\n",
    "        for i in range(n):\n",
    "            if self.predict(X_train[i]) == labels[i]:\n",
    "                count += 1\n",
    "\n",
    "        print(\"The decision tree is %d percent accurate on %d training data\" % ((count / n) * 100, n))\n",
    "\n",
    "        n = X_test.shape[0]\n",
    "\n",
    "        count = 0\n",
    "        for i in range(n):\n",
    "            if self.predict(X_test[i]) == y_test[i]:\n",
    "                count += 1\n",
    "\n",
    "        print(\"The decision tree is %d percent accurate on %d test data\" % ((count / n) * 100, n))\n",
    "\n",
    "        return count / n\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the tree\n",
    "Try different values of K (depth of tree a.k.a. number of features the tree will split on) and compare the performance. Which feature gives the largest information gain? Which feature is the least useful for the decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data imported\n",
      "For k= 1 :\n",
      "The decision tree is 84 percent accurate on 1097 training data\n",
      "The decision tree is 84 percent accurate on 275 test data\n",
      "[0, 0, 0]\n",
      "Largest information gain feature is: 0\n",
      "Least information gain feature is: 0 \n",
      "\n",
      "For k= 2 :\n",
      "The decision tree is 84 percent accurate on 1097 training data\n",
      "The decision tree is 83 percent accurate on 275 test data\n",
      "[0, 0, 0, 0, 0, 0, 0]\n",
      "Largest information gain feature is: 0\n",
      "Least information gain feature is: 0 \n",
      "\n",
      "For k= 3 :\n",
      "The decision tree is 84 percent accurate on 1097 training data\n",
      "The decision tree is 85 percent accurate on 275 test data\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Largest information gain feature is: 0\n",
      "Least information gain feature is: 0 \n",
      "\n",
      "For k= 4 :\n",
      "The decision tree is 85 percent accurate on 1097 training data\n",
      "The decision tree is 84 percent accurate on 275 test data\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Largest information gain feature is: 0\n",
      "Least information gain feature is: 0 \n",
      "\n",
      "For k= 5 :\n",
      "The decision tree is 85 percent accurate on 1097 training data\n",
      "The decision tree is 84 percent accurate on 275 test data\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Largest information gain feature is: 0\n",
      "Least information gain feature is: 0 \n",
      "\n",
      "For k= 6 :\n",
      "The decision tree is 86 percent accurate on 1097 training data\n",
      "The decision tree is 82 percent accurate on 275 test data\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Largest information gain feature is: 0\n",
      "Least information gain feature is: 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = import_data(split=0.8)\n",
    "\n",
    "def getfeature(node, features):\n",
    "    if node != None:\n",
    "        if node.feature != None :\n",
    "            features.append(node.feature)\n",
    "            getfeature(node.left, features)\n",
    "            getfeature(node.right, features)\n",
    "\n",
    "# Use Entropy\n",
    "for i in range(1, 7):\n",
    "    print(\"For k=\", i, \":\")\n",
    "    tree = DecisionTree(K=i, verbose=False)\n",
    "    tree.buildTree(X_train, y_train)\n",
    "    tree.homework_evaluate(X_train, y_train, X_test, y_test)\n",
    "    features = []\n",
    "    getfeature(tree.root, features)\n",
    "    print(features)\n",
    "    print(\"Largest information gain feature is:\", np.argmax(np.bincount(features)))\n",
    "    print(\"Least information gain feature is:\", np.argmin(np.bincount(features)), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k= 1 :\n",
      "The decision tree is 89 percent accurate on 1097 training data\n",
      "The decision tree is 89 percent accurate on 275 test data\n",
      "Largest information gain feature is: 0\n",
      "Least information gain feature is: 1 \n",
      "\n",
      "For k= 2 :\n",
      "The decision tree is 96 percent accurate on 1097 training data\n",
      "The decision tree is 95 percent accurate on 275 test data\n",
      "Largest information gain feature is: 0\n",
      "Least information gain feature is: 1 \n",
      "\n",
      "For k= 3 :\n",
      "The decision tree is 98 percent accurate on 1097 training data\n",
      "The decision tree is 97 percent accurate on 275 test data\n",
      "Largest information gain feature is: 0\n",
      "Least information gain feature is: 3 \n",
      "\n",
      "For k= 4 :\n",
      "The decision tree is 98 percent accurate on 1097 training data\n",
      "The decision tree is 98 percent accurate on 275 test data\n",
      "Largest information gain feature is: 0\n",
      "Least information gain feature is: 3 \n",
      "\n",
      "For k= 5 :\n",
      "The decision tree is 99 percent accurate on 1097 training data\n",
      "The decision tree is 98 percent accurate on 275 test data\n",
      "Largest information gain feature is: 0\n",
      "Least information gain feature is: 3 \n",
      "\n",
      "For k= 6 :\n",
      "The decision tree is 99 percent accurate on 1097 training data\n",
      "The decision tree is 98 percent accurate on 275 test data\n",
      "Largest information gain feature is: 0\n",
      "Least information gain feature is: 3 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use Gini\n",
    "for i in range(1, 7):\n",
    "    print(\"For k=\", i, \":\")\n",
    "    tree = DecisionTree(K=i, verbose=False)\n",
    "    tree.buildTree(X_train, y_train, metric = \"gini\")\n",
    "    tree.homework_evaluate(X_train, y_train, X_test, y_test)\n",
    "    features = []\n",
    "    getfeature(tree.root, features)\n",
    "    print(\"Largest information gain feature is:\", np.argmax(np.bincount(features)))\n",
    "    print(\"Least information gain feature is:\", np.argmin(np.bincount(features)), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get to know which feature has the largest information gain and which has the least, I collect the selected feature column for each node and try to find out which feature are selected most. As we can see from the above result, the first feature has the largest information gain, and the fourth feature has the least. Using Gini and entropy does not have a sigfinicant difference regarding the training and testing accuracy since the way of calculating uncertainty is quite similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ————————————————————————————————"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Decision Tree Exercise\n",
    "This section is designed to give you some exposure to typical preprocessing and allow you to run your decision tree code on another example. \n",
    "\n",
    "All of the preprocessing has been done for you — there's nothing you need to fill in, but it may be worthwhile to tinker with some of the pieces to make \n",
    "sure you understand how everything fits together. \n",
    "\n",
    "Otherwise, if your decision tree code works on the bank notes example, you should be able to run through this straight away. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Preprocessing\n",
    "To start, you'll need to download the data files from https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/.\n",
    "The file `breast-cancer.data` contains the actual data you'll need and the file `breast-cancer.names` gives some information about the researchers and the data types (it's probably worth looking at to give you a sense of what's going on).\n",
    "\n",
    "Note: If you try to open `breast-cancer.data` or `breast-cancer.names` directly, your computer might not know how to handle the file format. To view them, you need to change the file types from .data and .names to .txt — this can be accomplished by simply changing the file name from `breast-cancer.data` to `breast-cancer.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the data file into the notebook. All you need to do is put it in the same directory as the notebook and the cell below should locate the appropriate file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_directory = os.listdir()\n",
    "\n",
    "try:\n",
    "  cancer_files = [file for file in current_directory if 'cancer' in file]\n",
    "  data_file = [file for file in cancer_files if 'names' not in file][0]\n",
    "except IndexError:\n",
    "  print('The breast cancer files were not found. Please upload again.')\n",
    "  data_file = None\n",
    "\n",
    "if data_file:\n",
    "  print(f'The data file has been located as {data_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!! If the cell above returned a file that you don't recognize as the correct data file, you must go back to the upload step and ensure you have successfully uploaded your files before proceeding. !!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try to read the sample into our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "\n",
    "with open(data_file, 'r') as file:\n",
    "  samples = file.readlines()\n",
    "\n",
    "samples = [sample.split(',') for sample in samples]\n",
    "\n",
    "print(f'There are {len(samples)} samples in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start defining how we want to map our sample values to numeric features that can be used in the decision tree algorithm below. Let's first store our samples in a pandas DataFrame so that it'll be easier to view and work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "columns = ['class', 'age', 'menopause', 'tumor-size', 'inv-nodes', \n",
    "           'node-caps', 'deg-malig', 'breast', 'breast-quad', 'irradiat']\n",
    "\n",
    "samples_df = pd.DataFrame(samples, columns=columns)\n",
    "samples_df['irradiat'] = samples_df['irradiat'].str.replace('\\\\n', '')\n",
    "\n",
    "samples_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our DataFrame looks great! Now that we have the raw data stored in an interpretable way, it's good practice to make a copy before trying to encode everything — if something goes wrong, it'll be easy to just come back up here and reset the encoded DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_samples_df = samples_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following few cells, we're going to define the different types of variables that exist in our dataset and make sure we assign the appropriate type of encoding. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we'll define our binary variables (0,1) and create Python dictionaries to map the string labels to binary integer values. \n",
    "\n",
    "It's also helpful to store all of the columns and dictionaries in two lists so that we can easily reference them later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = ['class', 'breast', 'irradiat']\n",
    "\n",
    "class_map = {'no-recurrence-events': 0, 'recurrence-events': 1}\n",
    "breast_map = {'left': 0, 'right': 1}\n",
    "irrad_map = {'yes': 1, 'no': 0}\n",
    "\n",
    "binary_maps = [class_map, breast_map, irrad_map]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we've defined our ordinal variables (those that have obvious ordered structure). Instead of hard-coding the maps here, it's nice to have a function that can take any number of ordinal columns and instantly create all of the corresponding maps. The code below does that exactly, relying on the integer value of the first number in the range (i.e. '50-65') as the sorting key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_cols = ['age', 'tumor-size', 'inv-nodes', 'deg-malig']\n",
    "\n",
    "def first_val(x):\n",
    "  return eval(x.split('-')[0].replace(\"'\", ''))\n",
    "  \n",
    "ordinal_maps = {}\n",
    "\n",
    "for col in ordinal_cols:\n",
    "  uniques = sorted(list(encoded_samples_df[col].unique()), key=first_val)\n",
    "  val_map = {}\n",
    "  i = 1\n",
    "\n",
    "  for val in uniques:\n",
    "    val_map[val] = i\n",
    "    i += 1\n",
    "  \n",
    "  ordinal_maps[col] = val_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have several columns that take n > 2 discrete values. Binary encoding won't work here, so we'll use one-hot encoding. Just like in the cell immediately above, we'll use a function here to take an arbitrary number of columns and encode their unique values as one-hot vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_cols = ['menopause', 'breast-quad', 'node-caps']\n",
    "one_hot_maps = {}\n",
    "\n",
    "def one_hot_map(col, df):\n",
    "  one_hot = {}\n",
    "  unique_vals = list(df[col].unique())\n",
    "  one_hot_vecs = np.identity(len(unique_vals))\n",
    "\n",
    "  for i in range(len(unique_vals)):\n",
    "    one_hot[unique_vals[i]] = one_hot_vecs[i]\n",
    "\n",
    "  return one_hot\n",
    "\n",
    "for col in one_hot_cols:\n",
    "  one_hot_maps[col] = one_hot_map(col, encoded_samples_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of our encoding maps set up, we'll zip them all up into a master dictionary so that we can easily iterate through them on our `encoded_samples_df` DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_maps = dict(one_hot_maps, **ordinal_maps)\n",
    "for col, val_map in zip(binary_cols, binary_maps):\n",
    "  all_maps[col] = val_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, let's double check to make sure you've captured all of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_maps) != len(columns):\n",
    "  print(\"You're missing a map! Go back and double check that you didn't miss a column.\")\n",
    "else:\n",
    "  print(\"Looks like you successfully created all the maps! Nice work!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the moment of truth! Let's apply all of our encoding maps on the DataFrame to turn everything into useable features for our decision trees algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in columns:\n",
    "  encoded_samples_df[col] = encoded_samples_df[col].map(all_maps[col])\n",
    "\n",
    "encoded_samples_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check your `encoded_samples_df` here to make sure everything passes the sniff test! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming it all looks good, the final step is to break up our DataFrame into individual samples, unpack the nested arrays into their constituent values, and concatenate everything together into a final sample vector..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = list(encoded_samples_df.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_nested_arrays(vec):\n",
    "  final_vec = np.array([])\n",
    "\n",
    "  for e in vec:\n",
    "    if isinstance(e, int):\n",
    "      e = np.array([e])\n",
    "    final_vec = np.concatenate((final_vec, e))\n",
    "\n",
    "  return final_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vecs = []\n",
    "for e in sample_list:\n",
    "  final_vecs.append(unpack_nested_arrays(e))\n",
    "\n",
    "final_vecs = np.array(final_vecs, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The `final_vecs` variable should now point to a 286x19 numpy.ndarray containing all of our samples. Let's finally move on to the machine learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Decision Tree\n",
    "\n",
    "We'll run the decision tree process here again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_cancer_data(split=0.8, shuffle=True, CUTOFF=0, bins = 256):\n",
    "\n",
    "    if shuffle:\n",
    "      np.random.shuffle(final_vecs)\n",
    "    \n",
    "    num_samples = final_vecs.shape[0]\n",
    "    num_train_samples = int(num_samples*split)\n",
    "    \n",
    "    train_data, test_data = final_vecs[:num_train_samples, :], final_vecs[num_train_samples:, :]\n",
    "\n",
    "    X_train = train_data[:, 1:]\n",
    "    y_train = train_data[:, 0]\n",
    "    X_test = test_data[:, 1:]\n",
    "    y_test = test_data[:, 0]\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the final fit/evaluation code again, but this time using the breast cancer data. You should get somewhere around 0.75 accuracy for the decision tree on this dataset — not 100%, but not too bad for the humble decision tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = import_cancer_data(split=0.8)\n",
    "\n",
    "tree = DecisionTree(K=3, verbose=False)\n",
    "tree.buildTree(X_train, y_train)\n",
    "\n",
    "tree.homework_evaluate(X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
