{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import special\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the color map to be used for plotting.\n",
    "cmap = colors.LinearSegmentedColormap(\n",
    "    'red_blue_classes',\n",
    "    {'red': [(0, 1, 1), (1, 0.7, 0.7)],\n",
    "     'green': [(0, 0.7, 0.7), (1, 0.7, 0.7)],\n",
    "     'blue': [(0, 0.7, 0.7), (1, 1, 1)]})\n",
    "plt.cm.register_cmap(cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read the data from the problem 3 file.\n",
    "from numpy import genfromtxt\n",
    "my_data = genfromtxt('logistic_regression.csv', delimiter=',', skip_header=1)\n",
    "print(my_data.shape)\n",
    "X = my_data[:, [0, 1]]\n",
    "y = my_data[:, 2]\n",
    "\n",
    "# Divide the data into two classes for plotting.\n",
    "X0 = my_data[numpy.where(my_data[:, 2] == 0)]\n",
    "X1 = my_data[numpy.where(my_data[:, 2] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements the natural log of the logistic function on $w^T x$.\n",
    "def log_logistic_prob(iterate, x):\n",
    "    t = numpy.dot(iterate[1:], x) + iterate[0]\n",
    "    if t < -33:\n",
    "        return t\n",
    "    elif t < -18:\n",
    "        return t - numpy.exp(t)\n",
    "    elif t < 37:\n",
    "        return -numpy.log1p(numpy.exp(- t ))\n",
    "    else:\n",
    "        return -numpy.exp(-t)\n",
    "\n",
    "# Implements the logistic function on $w^T x$.\n",
    "def logistic_prob(iterate, x):\n",
    "    t = numpy.dot(iterate[1:], x) + iterate[0]\n",
    "    if t < -33.3:\n",
    "        return numpy.exp(t)\n",
    "    elif t <= -18:\n",
    "        return numpy.exp(t - numpy.exp(t))\n",
    "    elif t <= 37:\n",
    "        return numpy.exp(-numpy.log1p(numpy.exp(-t)))\n",
    "    else:\n",
    "        return numpy.exp(-numpy.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hUJ_acF1jAb"
   },
   "outputs": [],
   "source": [
    "# Evaluates the logistic function on a set of grid points.\n",
    "def logistic_prob_grid(iterate, grids):\n",
    "    return numpy.array([logistic_prob(iterate, x) for x in grids ])\n",
    "\n",
    "# Takes the logistic probability and thresholds to outputs a classification label.\n",
    "def logistic_pred(iterate, x):\n",
    "    return 1.0 if logistic_prob(iterate, x) > 0.5 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements the negative logistic regression objective. Modify me!\n",
    "def negative_log_likelihood(iterate, X, y, regularization, parameter):\n",
    "        \n",
    "    # The accumulated objective value.\n",
    "    obj_val = 0.0\n",
    "        \n",
    "    # Loop over each (x, y) pair.\n",
    "    for i, (x_vec, y) in enumerate(zip(X, y)):\n",
    "            \n",
    "        # Dot product $w^T x$.          \n",
    "        predict = iterate[0] + numpy.dot(iterate[1:], x_vec)\n",
    "            \n",
    "        # Accumulate the objective value contribution from this (x, y) pair.\n",
    "        obj_val += (- (1 - y) * predict + log_logistic_prob(iterate, x_vec) )\n",
    "        \n",
    "    # Subtract the regularization parameter.\n",
    "    return - obj_val + regularization * iterate[parameter]*iterate[parameter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements the logistic regression gradient. Modify me!\n",
    "def gradient_negative_log_likelihood(iterate, X, y, regularization, parameter):\n",
    "    gradient = numpy.zeros(3)\n",
    "    \n",
    "    # Loop over each (x, y) pair. \n",
    "    for i, (x_vec, y) in enumerate(zip(X, y)):\n",
    "        \n",
    "        # Dot product $w^T x$.            \n",
    "        predict = iterate[0] + numpy.dot(iterate[1:], x_vec)\n",
    "        \n",
    "        if predict > 0.0:\n",
    "            factor = ((y - 1) + y * numpy.exp(-predict)) / ( 1 + numpy.exp(-predict) )\n",
    "        else:\n",
    "            factor = ((y - 1) * numpy.exp(predict) + y ) / ( 1 + numpy.exp(predict) )\n",
    "        gradient[0] -= factor\n",
    "        gradient[1:] -= factor * x_vec\n",
    "        \n",
    "    # Regularize gradient.\n",
    "    gradient[parameter] += 2 * regularization * iterate[parameter]\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the data with the decision boundary.\n",
    "def plot_data(X, y, iterate, regularization, parameter):\n",
    "    y_pred = [logistic_pred(iterate, x) for x in X]\n",
    "    tp = (y == y_pred)  # True Positive\n",
    "    tp0, tp1 = tp[y == 0], tp[y == 1]\n",
    "    X0, X1 = X[y == 0], X[y == 1]\n",
    "    X0_tp, X0_fp = X0[tp0], X0[~tp0]\n",
    "    X1_tp, X1_fp = X1[tp1], X1[~tp1]\n",
    "\n",
    "    # class 0: dots\n",
    "    plt.scatter(X0_tp[:, 0], X0_tp[:, 1], marker='.', color='red')\n",
    "    plt.scatter(X0_fp[:, 0], X0_fp[:, 1], marker='x',\n",
    "                s=20, color='#990000')  # dark red\n",
    "\n",
    "    # class 1: dots\n",
    "    plt.scatter(X1_tp[:, 0], X1_tp[:, 1], marker='.', color='blue')\n",
    "    plt.scatter(X1_fp[:, 0], X1_fp[:, 1], marker='x',\n",
    "                s=20, color='#000099')  # dark blue\n",
    "\n",
    "    # class 0 and 1 : areas\n",
    "    nx, ny = 200, 200\n",
    "    x_min, x_max = (-10, 10)\n",
    "    y_min, y_max = (-10, 10)\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(-10, 10)\n",
    "    xx, yy = numpy.meshgrid(numpy.linspace(x_min, x_max, nx),\n",
    "                         numpy.linspace(y_min, y_max, ny))\n",
    "\n",
    "    Z = logistic_prob_grid(iterate, numpy.c_[xx.ravel(), yy.ravel()])    \n",
    "    Z = Z.reshape(xx.shape)\n",
    "    neg_log_likelihood = negative_log_likelihood(iterate, X, y, regularization, parameter)\n",
    "    plt.title(\n",
    "        'Negative log likelihood: ' + str(neg_log_likelihood))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap='red_blue_classes',\n",
    "                   norm=colors.Normalize(0., 1.), zorder=0)\n",
    "    \n",
    "    # Plot my linear decision boundary here!\n",
    "    x_data = numpy.linspace(x_min, x_max, nx)\n",
    "    plt.plot(x_data, (- x_data*iterate[1] - iterate[0])/iterate[2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for each different regularization value by calling your optimization routine for\n",
    "# different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "negative_log_likelihood() missing 1 required positional argument: 'parameter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-09b550ef161a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m res = minimize(\n\u001b[1;32m      2\u001b[0m     \u001b[0mnegative_log_likelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'BFGS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     jac = gradient_negative_log_likelihood, args=(X,y,numpy.array([0,0]),))\n\u001b[0m",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m   1001\u001b[0m     \u001b[0mfunc_calls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m     \u001b[0mold_fval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfprime\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: negative_log_likelihood() missing 1 required positional argument: 'parameter'"
     ]
    }
   ],
   "source": [
    "res = minimize(\n",
    "    negative_log_likelihood, [-1,1,2], method = 'BFGS',\n",
    "    jac = gradient_negative_log_likelihood, args=(X,y,numpy.array([0,0]),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Module 3 Lecture.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
