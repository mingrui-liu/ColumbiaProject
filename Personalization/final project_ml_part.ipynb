{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192609/192609 [00:02<00:00, 92573.43it/s]\n"
     ]
    }
   ],
   "source": [
    "## read in the business profile file\n",
    "\n",
    "line_count = len(open(\"business.json\").readlines())\n",
    "business_ids, categories, attr= [], [], []\n",
    "with open(\"business.json\") as f:\n",
    "    for line in tqdm(f, total=line_count):\n",
    "        blob = json.loads(line)\n",
    "        business_ids += [blob[\"business_id\"]]\n",
    "        categories += [blob[\"categories\"]]\n",
    "        attr += [blob[\"attributes\"]]\n",
    "business = pd.DataFrame(\n",
    "{\"business_id\": business_ids, \"category\": categories,\"attr\": attr}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Separate them into train, validation and test data and return indexes of the data.\n",
    "def make_selection(data_sample, train_size=0.8, val_size=0.2):\n",
    "    '''\n",
    "    Divide the dataset into training and validation dataset. \n",
    "    To avoid the cold start problem, we add the constraint on the training set\n",
    "    that it should contain at least one rating from all users and all movies\n",
    "    should be rated at least once.\n",
    "\n",
    "    data_sample - the data that we will split\n",
    "    train_size - the size of the training set in percentage\n",
    "    val_size - the size of the validation set in percentage\n",
    "    '''\n",
    "    indexes_selected = []\n",
    "    pd_data_grouped = data_sample.groupby('user_id')\n",
    "    for name, group in pd_data_grouped:\n",
    "        indexes_selected.append(np.random.choice(group.index,size=1, replace=False))\n",
    "\n",
    "    pd_data_grouped_item = data_sample.groupby('business_id')\n",
    "    for name, group in pd_data_grouped_item:\n",
    "        indexes_selected.append(np.random.choice(group.index,size=1, replace=False))\n",
    "    \n",
    "    indexes_selected = np.unique(np.asarray(indexes_selected))\n",
    "\n",
    "    num_train = int(data_sample.shape[0]*train_size)\n",
    "    num_validation = int(data_sample.shape[0]*val_size)\n",
    "\n",
    "    num_selected = indexes_selected.size\n",
    "\n",
    "    data_left = data_sample.drop(indexes_selected)\n",
    "\n",
    "    training_index = np.random.choice(data_left.index, size=(num_train-num_selected), replace=False)\n",
    "    validation_index = np.random.choice(data_left.drop(training_index).index, size=num_validation, replace=False)\n",
    "    \n",
    "    training_index = np.append(training_index, indexes_selected)\n",
    "\n",
    "    return training_index, validation_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_size(small_sample):\n",
    "    '''\n",
    "    Divide the dataset into training, validation and testing dataset\n",
    "    Return the subseted datasets.\n",
    "    \n",
    "    small_sample - the input of sample dataset\n",
    "    '''\n",
    "    training_index, validation_index = make_selection(small_sample, train_size=0.8, val_size = 0.2)\n",
    "    train_data = small_sample.loc[training_index,]\n",
    "\n",
    "    val_data = small_sample.loc[validation_index,]\n",
    "\n",
    "    #test_data = small_sample.loc[test_index, ['rating', 'movieId', 'userId']]\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##convert each attribute feature to a single word as new_attr\n",
    "def get_attr(x):\n",
    "    '''\n",
    "    Convert each attribute feature to a single word as new_attr \n",
    "    Return the new dataset\n",
    "    \n",
    "    x - the input of sample dataset\n",
    "    '''\n",
    "    l=[]\n",
    "    if x !=None:\n",
    "        for k in x.keys():\n",
    "            if x[k]==\"True\":\n",
    "                l.append(k.lower())\n",
    "    return ', '.join(map(str,l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_matrix(business):\n",
    "        '''\n",
    "    Calculate the cosine similarity matrix of item(business)\n",
    "    \n",
    "    business - business profile\n",
    "    \n",
    "        '''\n",
    "    # instantiating and generating the count matrix\n",
    "    count = CountVectorizer()\n",
    "    count_matrix = count.fit_transform(business[\"bag_of_words\"])\n",
    "\n",
    "    # generating the cosine similarity matrix\n",
    "    cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
    "\n",
    "    # creating a Series for the business_id  so they are associated to an ordered numerical\n",
    "    # list I will use later to match the indexes\n",
    "    indices = pd.Series(business['business_id'])\n",
    "    \n",
    "    return cosine_sim,indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendations(title,n = 5):\n",
    "    '''\n",
    "    function that takes in bisiness_id as input and returns \n",
    "    the top n = 5 recommended stores\n",
    "    The returned value is index of the recommendations and the \n",
    "    set of category it belongs to.\n",
    "    \n",
    "    title - bisiness_id need recommendations\n",
    "    n - number of recommendatios need\n",
    "    '''\n",
    "    recommended = []\n",
    "    return_index =[]\n",
    "    category = set()\n",
    "    idx = 1\n",
    "    # gettin the index of the business that matches the bisiness_id\n",
    "    if(title in list(indices)):\n",
    "        idx = indices[indices == title].index[0]\n",
    "\n",
    "    if(idx == 14967):\n",
    "        idx = 14966\n",
    "    # creating a Series with the similarity scores in descending order\n",
    "    score_series = pd.Series(cosine_sim[idx]).sort_values(ascending = False)\n",
    "\n",
    "    # getting the indexes of the 5 most similar bisiness\n",
    "    top_5_indexes = list(score_series.iloc[1:6].index)\n",
    "    \n",
    "    # populating the list with the titles of the best 5 matching bisiness_id\n",
    "    for i in top_5_indexes:\n",
    "\n",
    "        return_index.append(i)\n",
    "    \n",
    "        temp = list(business['category'])[i].split(\",\")\n",
    "        temp = list(map(lambda x:x.strip() , temp))\n",
    "        category.update(set(temp))\n",
    "        \n",
    "    return return_index,category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(user_id):\n",
    "    '''\n",
    "    Calculate the accuracy of the recommendations we have for a single user\n",
    "    by comparing the category of our recommendations with his or her actual interested \n",
    "    category in the test data.\n",
    "    The higher the accuracy the higher the overlap of the recommendations category and the \n",
    "    actual interested category.\n",
    "    Return the accuracy number\n",
    "    \n",
    "    user_id -- the user id of s single user\n",
    "    '''\n",
    "    ttt = train_data[train_data['user_id']==user_id]\n",
    "    ttt = ttt[ttt[\"rating\"]>3]\n",
    "  \n",
    "    recommend_cate_set = set()\n",
    "    def fun1(business_id):\n",
    "        category = recommendations(business_id)[1]\n",
    "        recommend_cate_set.update(category)\n",
    "        return recommend_cate_set\n",
    "    \n",
    "    list(map(lambda x:fun1(x),list(ttt[\"business_id\"])));\n",
    "    \n",
    "    true_cate = set()\n",
    "    def fun2(i):\n",
    "        temp = list(business[business[\"business_id\"] == i]['category'])[0].split(\",\")\n",
    "        temp = list(map(lambda x:x.strip() ,temp))\n",
    "        if (len(temp) >0):\n",
    "            true_cate.update(temp)\n",
    "        return true_cate\n",
    "    \n",
    "    true_list = list(set(ppp[\"business_id\"]) & set(business[\"business_id\"]))\n",
    "    list(map(lambda x:fun2(x),true_list))\n",
    "    \n",
    "    accuracy = 0\n",
    "    if(len(true_cate) != 0):\n",
    "        accuracy = len( recommend_cate_set & true_cate)/len(true_cate)\n",
    "        \n",
    "    \n",
    "    \n",
    "    return (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_serendipity(title1,title2):\n",
    "    '''\n",
    "    We care about the serendipity of our recomendtiosn\n",
    "    The amount of relevant information that is new to the user in a\n",
    "    recommendation\n",
    "    Return the serendipity score\n",
    "    \n",
    "    title1- The original business_id the user have rated \n",
    "    title2 -The recommendedbusiness_id\n",
    "    '''\n",
    "    t1_word = business.loc[business['business_id'] ==title1]['bag_of_words'].map(lambda x: x.split(','))\n",
    "    t1_word = set(t1_word.tolist()[0])\n",
    "    \n",
    "    t2_word = business.loc[business['business_id']==title2][\"bag_of_words\"].map(lambda x: x.split(','))\n",
    "    t2_word= set(t2_word.tolist()[0])\n",
    "    \n",
    "    recommend_len =len(t2_word)\n",
    "    existed_len = len(t1_word.intersection(t2_word))\n",
    "    serendipity_score = (recommend_len - existed_len)/recommend_len\n",
    "    \n",
    "    \n",
    "    return serendipity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_serendipity(title1,recommend):\n",
    "    '''\n",
    "    The overall serendipity of our recommendations\n",
    "    Return the serendipity score\n",
    "    \n",
    "    title1-The original business_id the user have rated \n",
    "    recommend - The list of recommendations we provide\n",
    "    \n",
    "    '''\n",
    "    recommend_id = business.loc[business['business_id'].isin(recommend)][\"business_id\"].tolist()\n",
    "    \n",
    "    recommend_serendipity = {}\n",
    "    for title2 in recommend_id:\n",
    "        recommend_serendipity[title2] = single_serendipity(title1,title2)\n",
    "        \n",
    "    return  recommend_serendipity  \n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "## yelp.csv is the small sample we subset from the review dataset, which include useres with 5 or more reviews\n",
    "sample = pd.read_csv('yelp.csv')\n",
    "\n",
    "## hold out user's final review (by date) as the test data and make the rest of them as train data\n",
    "sample.date = pd.to_datetime(sample.date)\n",
    "sample.dropna(axis=0,how='any');\n",
    "test=sample.sort_values(by=['user_id','date']).groupby(\"user_id\")['date'].max()\n",
    "test=pd.DataFrame(test)\n",
    "test= test.merge(sample,on=['user_id','date'],how='left')\n",
    "train=sample[~sample['Unnamed: 0'].isin(np.array(test['Unnamed: 0']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = train.user_id.values\n",
    "test  = test[test[\"user_id\"].isin(train_user)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "##filter busuness so we only care about business rated by user in sample\n",
    "##and the business_id ehich have info in business profile\n",
    "t1 = sample.business_id.values \n",
    "t2 = business.business_id.values\n",
    "t3 = np.intersect1d(t1,t2)\n",
    "\n",
    "business = business[business['business_id'].isin(t3)]\n",
    "sample = sample[sample['business_id'].isin(t3)]\n",
    "##split train and validation data of the train data we have \n",
    "train_data, val_data = random_sample_size(train)\n",
    "\n",
    "##reindex business id\n",
    "reindex = pd.Series(list(range(len(business))))\n",
    "business=business.set_index([reindex])\n",
    "\n",
    "##combine new_attr and category as the item profile\n",
    "business[\"new_attr\"]=business['attr'].map(lambda x: get_attr(x))\n",
    "business[\"bag_of_words\"]=business[\"category\"]+', '+business[\"new_attr\"]\n",
    "\n",
    "##manipulate bag_of_words and category\n",
    "##remove null data and convert string to lower case\n",
    "business[\"category\"] = business[\"category\"].str.lower()\n",
    "business = business[pd.notnull(business[\"category\"])]\n",
    "business['bag_of_words'] = business['bag_of_words'].str.lower()\n",
    "business = business[pd.notnull(business['bag_of_words'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate the cosine matrix\n",
    "cosine_sim,indices = cosine_similarity_matrix(business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "####This is a test code\n",
    "\n",
    "# business_id_1 = \"gnKjwL_1w79qoiV3IC_xQQ\"\n",
    "# business_id_2 = \"PZ-LZzSlhSe9utkQYU8pFg\"\n",
    "user_id = \"5JVY32_bmTBfIGpCCsnAfw\"\n",
    "\n",
    "# ttt = train_data[train_data['user_id']==user_id]\n",
    "# ttt = ttt[ttt[\"rating\"]>3]\n",
    "# ppp = val_data[val_data['user_id']==user_id]\n",
    "# recommend_cate_set = set()\n",
    "# #recommend_index = []\n",
    "# for business_id in ttt[\"business_id\"]:\n",
    "#     recommend_return_index,category = recommendations(business_id)\n",
    "#     recommend_cate_set.update(category)\n",
    "#     #recommend_index.extend(recommend_return_index)\n",
    "# true_cate = set()\n",
    "# #true_index = []\n",
    "# for i in ppp[\"business_id\"]:\n",
    "#     #print(i)\n",
    "#     #true_index_single = business[business[\"business_id\"] == i].index.values\n",
    "#     #true_index.extend(true_index_single)\n",
    "#     temp = list(business[business[\"business_id\"] == i]['category'])[0].split(\",\")\n",
    "#     temp = list(map(lambda x:x.strip() ,temp))\n",
    "#    # print(temp)\n",
    "#     true_cate.update(temp)\n",
    "# len(recommend_cate_set & true_cate)/len(true_cate)\n",
    "\n",
    "# accuracy(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The test result is the accuracy from validation data\n",
    "test_result = [accuracy(i) for i in val_data.user_id.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3899222409484094"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##The mean Accuracy of the validation result\n",
    "sum(test_result)/len(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The test result is the accuracy from the test data (last review)\n",
    "final_result = [accuracy(i) for i in test.user_id.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2696801015680017"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##The mean Accuracy of the test result\n",
    "sum(final_result)/len(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_user_bus_rating(user_id):\n",
    "    temp = train[train.user_id == user_id]\n",
    "    return (temp.business_id, temp.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the rating of user for their last visited business\n",
    "def predict(user_id,idx):\n",
    "    '''\n",
    "    predict the rating of user for their last visited business\n",
    "    \n",
    "    user_id - single user id\n",
    "    idx- the index of their last visited business in the similarity matrix of business profile\n",
    "    '''\n",
    "    single_user_bus,single_user_bus_ratings = single_user_bus_rating(user_id)\n",
    "\n",
    "    single_user_bus_idx = [indices[indices == i ].index[0] for i in single_user_bus]\n",
    "    single_user_bus_sim = [cosine_sim[idx][i] for i in single_user_bus_idx]\n",
    "    sum = 0\n",
    "    for i in range(len(single_user_bus_sim)):\n",
    "\n",
    "        sum = sum + single_user_bus_sim[i]*list(single_user_bus_ratings)[i]\n",
    "    return sum/len(single_user_bus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE\n",
    "def getMSE(prediction, true_val):\n",
    "    '''\n",
    "    Calculate the MSE of true value and predicted value\n",
    "    \n",
    "    prediction - the pridicted rating of test_user\n",
    "    true_val - the true rating of the test_user\n",
    "    '''\n",
    "    # MSE on the train data\n",
    "    mse = np.nanmean(((true_val -  prediction) ** 2))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcRMSE(test):\n",
    "    '''\n",
    "    Calculate RMSE for the test data\n",
    "\n",
    "    '''\n",
    "    \n",
    "    mse_sum = 0\n",
    "    for user_id in test.user_id:\n",
    "        test_bus = test[test.user_id == user_id].business_id\n",
    "        idx = indices[indices == list(test_bus)[0]].index[0]\n",
    "        test_rating = test[test.user_id == user_id].rating\n",
    "        if (idx ==14967):\n",
    "            idx = 14966\n",
    "        predition = predict(user_id,idx)\n",
    "        mse = getMSE(predition,test_rating)\n",
    "        mse_sum += mse\n",
    "    RMSE = math.sqrt(mse_sum / len(test.user_id))\n",
    "    return RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0383425605220977"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcRMSE(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.695398142213499"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcRMSE(val_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
